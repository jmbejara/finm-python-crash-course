{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J09tr3ds09ky"
   },
   "source": [
    "# 1.3 Overview of the PyData Stack\n",
    "\n",
    "This notebook is designed to give you a brief overview of the PyData stack: a collection\n",
    "of Python packages that are used for data science. Here I introduce the most\n",
    "important packages in the PyData stack: NumPy, SciPy, Pandas, Matplotlib, StatsModels, and scikit-learn. \n",
    "I'll also show you some examples of Seaborn and iPyWidgets.\n",
    "\n",
    "I'll also demonstrate some features of Jupyter Notebooks in VS Code.\n",
    "Although Jupyter Notebooks are not always the right medium for your code, it will also demonstrate some of the features of Jupyter and Jupyter Notebooks that make them useful for data exploration and visualization.\n",
    "\n",
    "IMPORTANT NOTE: This is just meant to be a whirlwind overview of the important Python packages for data science. It's just an overview. We'll dig into the details over the next few class sessions. The point of this notebook is just to showcase what CAN be done and to make sure that you can download the code for the course and run it in VS Code. We'll learn how to write the code below over the next few weeks and throughout the next quarter. **After getting a look at what's possible in this notebook, we'll start from the very basics in the next notebook.** \n",
    "\n",
    "You can try running this code yourself. If you don't have VS Code installed, please do so ASAP. In a pinch, you can run this in Google Colaboratory: `https://colab.research.google.com/` You can open this notebook directly in Google Colaboratory by clicking here: [Open in Google Colaboratory.](https://colab.research.google.com/github/jmbejara/finm-python-crash-course/blob/main/src/01_python_for_data_science_demo.ipynb)\n",
    "\n",
    "This notebook will start by setting up the environment and then will demonstrate some examples from NumPy, SciPy, Pandas, Matplotlib, and StatsModels (with some bonus examples from Seaborn and iPyWidgets.) This collection of packages represent the foundation of what is called the PyData stack (ecosystem):\n",
    "\n",
    "<div>\n",
    "<img src=\"http://chris35wills.github.io/courses/pydata_stack.png\" width=\"1000\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgL_HaFX09k4"
   },
   "source": [
    "\n",
    "## 0. Set up Environment\n",
    "\n",
    "- Let's first make sure that everyone was able to download the code from GitHub. Please raise your hand if you are not able to open the project folder in VS Code.\n",
    "\n",
    "- Now, before we start, we need to set up our environment. We need to install the packages that we need (if they aren't installed already) and then we need to load the packages into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install required packages, but do automatically only if running within Google Colaboratory\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install plotly==5.9.0\n",
    "else:\n",
    "  print(\"Be sure to install the required packages manually if not in Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOKX9_RL09k8"
   },
   "source": [
    "## 1. NumPy\n",
    "\n",
    "NumPy a library designed to add support \"for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\"\n",
    "\n",
    "A good beginner tutorial can be found on the official NumPy website here: https://numpy.org/doc/stable/user/absolute_beginners.html\n",
    "\n",
    "The basic functionality of NumPy is the efficient management of arrays, with syntax as follows:\n",
    "<div>\n",
    "<img src=\"https://fgnt.github.io/python_crashkurs_doc/_images/numpy_array_t.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([\n",
    "    [1, 0], \n",
    "    [0, 1]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ksbfWdl09lA"
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/Matrix_multiplication_qtl1.svg\" width=\"1000\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array((1,2))\n",
    "x = np.linalg.solve(A, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval, eigvec = np.linalg.eig(A)\n",
    "eigval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oy8f0YOD09lC"
   },
   "source": [
    "## 2. SciPy\n",
    "\n",
    "SciPy is a library \"used for scientific computing and technical computing. SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.\"\n",
    "\n",
    "For example, consider calculating the following integral.\n",
    "$$\n",
    "\\int_0^1 a x^2 + b \\, d x\n",
    "$$\n",
    "\n",
    "<font color='red'>DISCUSS: How did I make typeset the above equation in this Jupyter notebook? What about the images above?</font>\n",
    "  - How did I create the section headers?\n",
    "  - How can I export this notebook into a PDF report? What about an HTML report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "a = 2\n",
    "b = 1\n",
    "def integrand(x):\n",
    "    return a*x**2 + b\n",
    "I, err = quad(integrand, 0, 1)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXUSbyiz09lD"
   },
   "source": [
    "## 3. Matplotlib\n",
    "\n",
    "Matplotlib is the most plotting library for Python. Even other plotting libraries build off of Matplotlib as a foundation. Even if you use other plotting libraries, it is important to understand the basics of Matplotlib.\n",
    "\n",
    "As an example, consider the function\n",
    "$$\n",
    "y = x_0 \\exp(-x_2 \\cdot t) + x_1 \\exp(-x_3 \\cdot t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1, 1, 0])\n",
    "yfunc = lambda t: x[0] * np.exp(-x[2]* t) + x[1] * np.exp(-x[3] * t)\n",
    "t_grid = np.linspace(0, 2, 100)\n",
    "y = yfunc(t_grid)\n",
    "plt.plot(t_grid, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HADy3tiY09lE"
   },
   "source": [
    "Now, let's consider some fancier examples. We'll even use some Seaborn code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/examples/errorband_lineplots.html\n",
    "# Load an example dataset with long-form data\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fmri = sns.load_dataset(\"fmri\")\n",
    "\n",
    "# Plot the responses for different events and regions\n",
    "sns.lineplot(x=\"timepoint\", y=\"signal\",\n",
    "             hue=\"region\", style=\"event\",\n",
    "             data=fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/examples/layered_bivariate_plot.html\n",
    "# sns.set_theme(style=\"dark\")\n",
    "\n",
    "# Simulate data from a bivariate Gaussian\n",
    "n = 10000\n",
    "mean = [0, 0]\n",
    "cov = [(2, .4), (.4, .2)]\n",
    "rng = np.random.RandomState(0)\n",
    "x, y = rng.multivariate_normal(mean, cov, n).T\n",
    "\n",
    "# Draw a combo histogram and scatterplot with density contours\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.scatterplot(x=x, y=y, s=5, color=\".15\")\n",
    "sns.histplot(x=x, y=y, bins=50, pthresh=.1, cmap=\"mako\")\n",
    "sns.kdeplot(x=x, y=y, levels=5, color=\"w\", linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/examples/timeseries_facets.html\n",
    "flights = sns.load_dataset(\"flights\")\n",
    "\n",
    "# Plot each year's time series in its own facet\n",
    "g = sns.relplot(\n",
    "    data=flights,\n",
    "    x=\"month\", y=\"passengers\", col=\"year\", hue=\"year\",\n",
    "    kind=\"line\", palette=\"crest\", linewidth=4, zorder=5,\n",
    "    col_wrap=3, height=2, aspect=1.5, legend=False,\n",
    ")\n",
    "\n",
    "# Iterate over each subplot to customize further\n",
    "for year, ax in g.axes_dict.items():\n",
    "\n",
    "    # Add the title as an annotation within the plot\n",
    "    ax.text(.8, .85, year, transform=ax.transAxes, fontweight=\"bold\")\n",
    "\n",
    "    # Plot every year's time series in the background\n",
    "    sns.lineplot(\n",
    "        data=flights, x=\"month\", y=\"passengers\", units=\"year\",\n",
    "        estimator=None, color=\".7\", linewidth=1, ax=ax,\n",
    "    )\n",
    "\n",
    "# Reduce the frequency of the x axis ticks\n",
    "ax.set_xticks(ax.get_xticks()[::2])\n",
    "\n",
    "# Tweak the supporting aspects of the plot\n",
    "g.set_titles(\"\")\n",
    "g.set_axis_labels(\"\", \"Passengers\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with:\n",
    "# conda install -c plotly plotly=5.9.0\n",
    "# or\n",
    "# pip install plotly==5.9.0\n",
    "\n",
    "df = px.data.stocks()\n",
    "fig = px.line(df, x='date', y=\"GOOG\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j63HvaTz09lG"
   },
   "source": [
    "## 4. Pandas\n",
    "\n",
    "From Wikipedia, \"`pandas` is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. The name is derived from the term \"panel data\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. Its name is a play on the phrase \"Python data analysis\" itself. Wes McKinney started building what would become pandas at AQR Capital while he was a researcher there from 2007 to 2010.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {\n",
    "    'California': 423967, \n",
    "    'Texas': 695662, \n",
    "    'New York': 141297,\n",
    "    'Florida': 170312, \n",
    "    'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "\n",
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user'\n",
    "users = pd.read_table(url, sep='|', index_col='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9Ku8QcY1gMM"
   },
   "source": [
    "## 5. StatsModels\n",
    "\n",
    "\"`statsmodels` is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=dat).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 100\n",
    "X = np.random.random((nobs, 2))\n",
    "X = sm.add_constant(X)\n",
    "beta = [1, .1, .5]\n",
    "e = np.random.random(nobs)\n",
    "y = np.dot(X, beta) + e\n",
    "\n",
    "# Fit regression model\n",
    "results = sm.OLS(y, X).fit()\n",
    "\n",
    "# Inspect the results\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. scikit-learn\n",
    "\n",
    "ScitKit Learn is a machine learning library for Python. It is built on NumPy, SciPy, and Matplotlib. It is designed to be easy to use and to work well with the other packages in the PyData stack. Although there are many other machine learning libraries for Python, scikit-learn is the most popular and is the best place to start if you are new to machine learning.\n",
    "\n",
    "\n",
    "In the example below, the authors\n",
    "start by creating a simple dataset with two features. They fit a PCA estimator to display the two principal components of this dataset, i.e. the two directions that explain the most variance in the data.\n",
    "\n",
    "From here: https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from here:\n",
    "# https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "n_samples = 500\n",
    "cov = [[3, 3], [3, 4]]\n",
    "X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    comp = comp * var  # scale component by its variance explanation power\n",
    "    plt.plot(\n",
    "        [0, comp[0]],\n",
    "        [0, comp[1]],\n",
    "        label=f\"Component {i}\",\n",
    "        linewidth=5,\n",
    "        color=f\"C{i + 2}\",\n",
    "    )\n",
    "plt.gca().set(\n",
    "    aspect=\"equal\",\n",
    "    title=\"2-dimensional dataset with principal components\",\n",
    "    xlabel=\"first feature\",\n",
    "    ylabel=\"second feature\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usHlQ_zd09lH"
   },
   "source": [
    "## 7. IPyWidgets\n",
    "\n",
    "### 7.1 Lorenz Attractor: Lorenz System of Differential Equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import cnames\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lorenz(N=10, angle=0.0, max_time=4.0, sigma=10.0, beta=8./3, rho=28.0):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0, 0, 1, 1], projection='3d')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # prepare the axes limits\n",
    "    ax.set_xlim((-25, 25))\n",
    "    ax.set_ylim((-35, 35))\n",
    "    ax.set_zlim((5, 55))\n",
    "    \n",
    "    def lorenz_deriv(tup, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "        x, y, z = tup\n",
    "        \"\"\"Compute the time-derivative of a Lorentz system.\"\"\"\n",
    "        return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "    # Choose random starting points, uniformly distributed from -15 to 15\n",
    "    np.random.seed(1)\n",
    "    x0 = -15 + 30 * np.random.random((N, 3))\n",
    "\n",
    "    # Solve for the trajectories\n",
    "    t = np.linspace(0, max_time, int(250*max_time))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0i, t)\n",
    "                      for x0i in x0])\n",
    "    \n",
    "    # choose a different color for each trajectory\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, N))\n",
    "\n",
    "    for i in range(N):\n",
    "        x, y, z = x_t[i,:,:].T\n",
    "        lines = ax.plot(x, y, z, '-', c=colors[i])\n",
    "        plt.setp(lines, linewidth=2)\n",
    "\n",
    "    ax.view_init(30, angle)\n",
    "    plt.show()\n",
    "\n",
    "    return t, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x_t = solve_lorenz(angle=0, N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = interactive(solve_lorenz, angle=(0.,360.), N=(0,50), sigma=(0.0,50.0), rho=(0.0,50.0))\n",
    "## Uncomment below for interactive viewing\n",
    "# display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn5V9OLk09lI"
   },
   "source": [
    "### 7.2 Mean-Variance Portfolio Analysis\n",
    "\n",
    "Suppose that you have two different assets that you could invest in, asset 0 or asset 1. Let $R_0$ and $R_1$ be random variables representing the gross return on asset 0 and asset 1, respectively. Let the return of each have means $\\mu_0$ and $\\mu_1$ and standard deviations $\\sigma_0$ and $\\sigma_1$. Let $w$ be portfolio weight allocated to asset 1. Then, the return on the portfolio is\n",
    "\n",
    "$$\n",
    "R = (1-w) R_0 + w R_1.\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[R_0] = \\mu_0\n",
    "$$\n",
    "$$\n",
    "E[R_1] = \\mu_1\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "std(R_0) &= \\sigma_0 \\\\\n",
    "std(R_1) &= \\sigma_1\n",
    "\\end{align}\n",
    "\n",
    "Now, what about the mean and standard devation of $R$?\n",
    "\n",
    "$$\n",
    "E[R] = (1-w) \\mu_0 + w \\mu_1.\n",
    "$$\n",
    "\n",
    "$$\n",
    "% std(R) = \\, ? %\\sqrt{(1-w)^2 \\sigma_0^2 + w^2 \\sigma_1^2 + 2 w (1-w) \\rho \\sigma_0 \\sigma_1}\n",
    "std(R) = \\sqrt{(1-w)^2 \\sigma_0^2 + w^2 \\sigma_1^2 + 2 w (1-w) \\rho \\sigma_0 \\sigma_1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_portfolio(weight_1=.5, mu0=2.0, mu1=8.0, sd0=5.0, sd1=10.0, corr=0.0, Rf=1.0):\n",
    "    \n",
    "    #calculate portfolio properties\n",
    "    expected_return = lambda a: a * mu1 + (1-a) * mu0\n",
    "    add_sd = lambda a: np.sqrt((1-a)**2 * sd0**2 + a**2 * sd1**2 \n",
    "                            + 2*a*(1-a)*corr*sd0 * sd1)\n",
    "    \n",
    "    #calculate frontier\n",
    "    weight_1_range = np.linspace(-2.0, 2.0, 100)\n",
    "    y = expected_return(weight_1_range)\n",
    "    x = add_sd(weight_1_range)\n",
    "    \n",
    "    #Plot frontier and points\n",
    "    XRIGHT = 15\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.plot([sd0, sd1], [mu0, mu1], 'o')\n",
    "    ax.axis([0, XRIGHT, -2, 10])\n",
    "    ax.plot([add_sd(weight_1)], [expected_return(weight_1)], 'o')\n",
    "    plt.xlabel('standard deviation')\n",
    "    plt.ylabel('expected return')\n",
    "    \n",
    "    if corr < 1:\n",
    "        #calculate risk-free rate frontier\n",
    "        one_vec = np.array([[1,1]]).T\n",
    "        Sigma = np.array([[sd0**2, sd0 * sd1 * corr],\n",
    "                          [sd0 * sd1 * corr, sd1**2]])\n",
    "        mu_vec = np.array([[mu0, mu1]]).T\n",
    "        #A = np.linalg.inv(Sigma) @ (mu_vec - one_vec * Rf)\n",
    "        A = np.linalg.solve(Sigma, (mu_vec - one_vec * Rf))\n",
    "        weights_vec = A * (one_vec.T @ A)**(-1)\n",
    "        Rt = mu_vec.T @ weights_vec\n",
    "        sd_t = np.sqrt(weights_vec.T @ Sigma @ weights_vec)\n",
    "        slope = (Rt - Rf) / (sd_t - 0)    \n",
    "        ax.plot([0, sd_t.item(), (sd_t + XRIGHT).item()], [Rf, Rt.item(), (Rt + XRIGHT * slope).item() ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portfolio(weight_1=.5, mu0=4.0, mu1=8.0, sd0=5.0, sd1=10.0, corr=0.0, Rf=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment below for interactive viewing\n",
    "# interact(plot_portfolio, weight_1=(-1.0, 2.0, .001), corr=(-1.0, 1.0, .01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCUSS: The example above demonstrate what is meant by a set of efficient portfolios? What do you think is meant by an efficient portfolio?**\n",
    "\n",
    "For more information about calculating the tangency portfolio, see here: https://bookdown.org/compfinezbook/introcompfinr/Efficient-portfolios-of.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
